{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "#++++++++++++++++++++++++++++++++++++++++++++++\n",
    "# Before running the script, edit \n",
    "# 'SET HYPERPARAMETERS' \n",
    "# - the rest is automated\n",
    "#++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "##################\n",
    "### IMPORT MODULES\n",
    "##################\n",
    "\n",
    "### System\n",
    "import sys\n",
    "import os\n",
    "\n",
    "### I/O\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "### General Processing\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import math\n",
    "import collections\n",
    "from collections import OrderedDict\n",
    "\n",
    "## DECOMPOSITION\n",
    "from sklearn.decomposition import NMF\n",
    "from scipy.linalg import svd\n",
    "\n",
    "### NLU\n",
    "from ibm_watson import NaturalLanguageUnderstandingV1 as NaLaUn\n",
    "from ibm_watson.natural_language_understanding_v1 import Features, CategoriesOptions,ConceptsOptions,EntitiesOptions,KeywordsOptions,RelationsOptions,SyntaxOptions\n",
    "\n",
    "### Presentation / apps\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import dash\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "import dash_table\n",
    "import dash_table.FormatTemplate as FormatTemplate\n",
    "from dash_table.Format import Sign\n",
    "from dash.dependencies import Input, Output\n",
    "\n",
    "## GENERAL FUNCTIONS \n",
    "### NORMALIZATION\n",
    "#### Statistic normalization - subtract mean, scale by standard deviation\n",
    "def norm_stat(vec, weights = False):\n",
    "    '''\n",
    "    Normalizes a vector v-v.mean())/v.std() \n",
    "    '''\n",
    "    if weights:\n",
    "        return  np.mean(abs(vec - vec.mean()))  \n",
    "    return (vec-vec.mean())/vec.std()\n",
    "\n",
    "#### Algebraic normalization - dot product\n",
    "def norm_dot(vec, weights = False):\n",
    "    '''\n",
    "    Normalizes a vector - dot product: v @ v = 1\n",
    "    '''\n",
    "    if weights:\n",
    "        return  np.sqrt(vec @ vec)\n",
    "    \n",
    "    return vec / np.sqrt(vec @ vec)\n",
    "\n",
    "#### Algebraic normalization - dot product\n",
    "def norm_sum(vec, weights = False):\n",
    "    '''\n",
    "    Normalizes a vector - sum: v.sum = 1\n",
    "    '''\n",
    "    if weights:\n",
    "        return  vec.sum()\n",
    "    \n",
    "    return vec / vec.sum()\n",
    "\n",
    "#### Scaled Normalization -\n",
    "def scale(vec, weights = False):\n",
    "    '''\n",
    "    Normalizes a vector: v.min = 0, v.max = 1\n",
    "    '''\n",
    "    stop_divide_by_zero = 0.00000001\n",
    "    if weights:\n",
    "        return (vec.max()-vec.min() + stop_divide_by_zero)\n",
    "    return (vec-vec.min())/(vec.max()-vec.min() + stop_divide_by_zero)\n",
    "def cleanup_chars(string,char_list = ('\\n',' ')):\n",
    "    result = string\n",
    "    for char in char_list:\n",
    "        result = result.replace(char,'')\n",
    "    return result\n",
    "\n",
    "##########################################\n",
    "### IBM-WATSON/NLU API-KEY (DON'T EDIT)\n",
    "##########################################\n",
    "# The script asks for the API key when run. \n",
    "# Do NOT save API-Keys in the code. \n",
    "\n",
    "local_dir_exists = os.path.exists('.local')\n",
    "if not local_dir_exists:\n",
    "    os.mkdir('.local')\n",
    "    \n",
    "credentials_exists = os.path.exists('.local/crd.env')\n",
    "if not credentials_exists:\n",
    "    print('Credentials needed for https://cloud.ibm.com/catalog/services/natural-language-understanding )')\n",
    "    apikey = input(prompt='API-Key?')\n",
    "    apiurl = input(prompt='API-URL?')\n",
    "    crd = open('.local/crd.env','w')\n",
    "    crd.write(  'NATURAL_LANGUAGE_UNDERSTANDING_IAM_APIKEY='+apikey)\n",
    "    crd.write('\\nNATURAL_LANGUAGE_UNDERSTANDING_URL='       +apiurl)  \n",
    "    \n",
    "\n",
    "# dian_pkl_file = PATH['results']+'all_dictations_nlu.pkl'  \n",
    "# dian_pkl_exists = os.path.exists(dian_pkl_file)\n",
    "# if 'apikey' not in locals():\n",
    "#     apikey = input(prompt='API-Key? ( https://cloud.ibm.com/catalog/services/natural-language-understanding )')  \n",
    "\n",
    "\n",
    "# # MATRIX-FACTORIZATION: DIMENSIONALITY REDUCTION & ARCHETYPING\n",
    "\n",
    "# ## CLUSTER FEATURES INTO OCCUPATION CATEGORIES\n",
    "# ## Use non-zero matrix factorization for clustering\n",
    "# ## Use singular value decomposition first state for determining overall similarity\n",
    "\n",
    "\n",
    "class Archetypes:\n",
    "    '''\n",
    "    Archetypes: Performs NMF of order n on X and stores the result as attributes. \n",
    "    Archetypes are normalized: cosine similarity a(i) @ a(i) = 1. \n",
    "    Atributes:\n",
    "        my_archetypes.n         - order / number of archetypes\n",
    "        my_archetypes.X         - input matrix\n",
    "        \n",
    "        my_archetypes.model     - NMF model \n",
    "        my_archetypes.w         - NMF w-matrix \n",
    "        my_archetypes.h         - NMF h-matrix\n",
    "        \n",
    "        my_archetypes.o         - occupations x archetypes matrix (from w-matrix)\n",
    "        my_archetypes.on        - occupations x normalized archetypes matrix (from w-matrix) - SOCP number as index. \n",
    "        my_archetypes.occ       - occupations x normalized archetypes matrix - Occupation names as index\n",
    "        \n",
    "        my_archetypes.f         - features x archetypes matrix (from h-matrix)\n",
    "        my_archetypes.fn        - features x normalized archetypes matrix\n",
    "        \n",
    "    '''\n",
    "    def __init__(self,X,n,norm = norm_dot):\n",
    "        self.n = n\n",
    "        self.X = X\n",
    "        self.model = NMF(n_components=n, init='random', random_state=0, max_iter = 1000, tol = 0.0000001)\n",
    "        self.w = self.model.fit_transform(self.X)\n",
    "        self.o = pd.DataFrame(self.w,index=self.X.index)\n",
    "        self.on = self.o.T.apply(norm).T\n",
    "        self.occ = self.on.copy()\n",
    "        self.occ['Occupations'] = self.occ.index\n",
    "#        self.occ['Occupations'] = self.occ['Occupations'].apply(onet_socp_name)\n",
    "        self.occ = self.occ.set_index('Occupations')\n",
    "        self.h = self.model.components_\n",
    "        self.f = pd.DataFrame(self.h,columns=X.columns)\n",
    "        self.fn =self.f.T.apply(norm).T\n",
    "        self.plot_occupations_dic ={}\n",
    "        self.plot_features_dic ={}\n",
    "\n",
    "        \n",
    "    def plot_features(self,fig_scale = (1,3.5),metric='cosine', method = 'single',vertical = False): \n",
    "        '''\n",
    "        Plot Archetypes as x and features as y. \n",
    "        Utilizes Seaborn Clustermap, with hierarchical clustering along both axes. \n",
    "        This clusters features and archetypes in a way that visualizes similarities and diffferences\n",
    "        between the archetypes. \n",
    "        \n",
    "        Archetypes are normalized (cosine-similarity): dot product archetype[i] @ archetype[i] = 1.\n",
    "        The plot shows intensities (= squared feature coefficients) so that the sum of intensities = 1.  \n",
    "\n",
    "        fig_scale: default values (x/1, y/3.5) scales the axes so that all feature labels are included in the plot.\n",
    "        \n",
    "        For other hyperparameters, see seaborn.clustermap\n",
    "     \n",
    "        '''\n",
    "        param = (fig_scale,metric,method,vertical)\n",
    "        if param in self.plot_features_dic.keys():\n",
    "            fig = self.plot_features_dic[param]\n",
    "            return fig.fig\n",
    "\n",
    "        df = np.square(self.fn)\n",
    "\n",
    "        if vertical:\n",
    "            fig = sns.clustermap(df.T,robust = True, z_score=1,figsize=(\n",
    "                self.n/fig_scale[0],self.X.shape[1]/fig_scale[1]),method = method,metric = metric)        \n",
    "        else: # horizontal\n",
    "            fig = sns.clustermap(df,robust = True, z_score=0,figsize=(\n",
    "                self.X.shape[1]/fig_scale[1],self.n/fig_scale[0]),method = method,metric = metric)        \n",
    "        self.features_plot = fig\n",
    "        return fig\n",
    "\n",
    "\n",
    "    def plot_occupations(self,fig_scale = (1,3.5),metric='cosine', method = 'single',vertical = False):\n",
    "        '''\n",
    "        Plot Archetypes as x and occupations as y. \n",
    "        Utilizes Seaborn Clustermap, with hierarchical clustering along both axes. \n",
    "        This clusters occupations and archetypes in a way that visualizes similarities and diffferences\n",
    "        between the archetypes. \n",
    "        \n",
    "        Occupations are normalized (cosine-similarity): dot product occupation[i] @ occupation[i] = 1.\n",
    "        The plot shows intensities (= squared feature coefficients) so that the sum of intensities = 1.  \n",
    "\n",
    "        fig_scale: default values (x/1, y/3.5) scales the axes so that all feature labels are included in the plot.\n",
    "        \n",
    "        For other hyperparameters, see seaborn.clustermap\n",
    "     \n",
    "        '''\n",
    "        param = (fig_scale,metric,method,vertical)\n",
    "        if param in self.plot_occupations_dic.keys():\n",
    "            fig = self.plot_occupations_dic[param]\n",
    "            #return\n",
    "            return fig.fig\n",
    "\n",
    "        df = np.square(self.occ)\n",
    "        if vertical:\n",
    "            fig = sns.clustermap(df, figsize=(\n",
    "                self.n/fig_scale[0],self.X.shape[0]/fig_scale[1]),method = method,metric = metric)\n",
    "        else: # horizontal\n",
    "            fig = sns.clustermap(df.T, figsize=(\n",
    "                self.X.shape[0]/fig_scale[1],self.n/fig_scale[0]),method = method,metric = metric)\n",
    "        self.plot_occupations_dic[param] = fig\n",
    "        #return\n",
    "        return fig.fig\n",
    "\n",
    "\n",
    "class Svd:\n",
    "    ''''\n",
    "    Singular value decomposition-as-an-object\n",
    "        my_svd = Svd(X) returns\n",
    "        my_svd.u/.s/.vt – U S and VT from the Singular Value Decomposition (see manual)\n",
    "        my_svd.f        – Pandas.DataFrame: f=original features x svd_features\n",
    "        my_svd.o        - Pandas.DataFrame: o=occupations x svd_features\n",
    "        my_svd.volume(keep_volume) \n",
    "                        - collections.namedtuple ('dotted dicionary'): \n",
    "                          Dimensionality reduction. keeps 'keep_volume' of total variance\n",
    "                          \n",
    "                          \n",
    "    '''\n",
    "    def __init__(self,X):\n",
    "        self.u,self.s,self.vt = svd(np.array(X))\n",
    "        self.f = pd.DataFrame(self.vt,columns=X.columns)\n",
    "        self.o = pd.DataFrame(self.u,columns=X.index)\n",
    "        \n",
    "    def volume(self,keep_volume):\n",
    "        ''' \n",
    "        Dimensionality reduction, keeps 'keep_volume' proportion of original variance\n",
    "        Type: collections.namedtuple ('dotted dictionary')\n",
    "        Examples of usage:\n",
    "        my_svd.volume(0.9).s - np.array: eigenvalues for 90% variance \n",
    "        my_svd.volume(0.8).f - dataframe: features for 80% variance\n",
    "        my_svd.volume(0.5).o - dataframe: occupations for 50% variance      \n",
    "        '''\n",
    "        dotted_dic = collections.namedtuple('dotted_dic', 's f o')\n",
    "        a1 = self.s.cumsum()\n",
    "        a2 = a1/a1[-1]\n",
    "        n_max = np.argmin(np.square(a2 - keep_volume))\n",
    "        cut_dic = dotted_dic(s= self.s[:n_max],f= self.f.iloc[:n_max], o= self.o.iloc[:n_max])\n",
    "        return cut_dic\n",
    "        \n",
    "\n",
    "\n",
    "##########################\n",
    "## SET HYPERPARAMATERS\n",
    "#### edit below ##########\n",
    "\n",
    "# Import credentials\n",
    "cred = open('.local/crd.env','r').read()\n",
    "apikey,apiurl = cred.replace('NATURAL_LANGUAGE_UNDERSTANDING_IAM_APIKEY=','').replace(\n",
    "                            'NATURAL_LANGUAGE_UNDERSTANDING_URL=','').split()\n",
    "\n",
    "PATH = {}\n",
    "PATH['data']    = '../data/Documents/'\n",
    "PATH['results'] = './Watson-nlu-results/'\n",
    "\n",
    "NLU = {}\n",
    "NLU['apikey']         = apikey\n",
    "NLU['apiurl']         = apiurl\n",
    "NLU['version']        = '2019-07-12'\n",
    "NLU['features']       = Features(\n",
    "                        categories= CategoriesOptions(limit=4),\n",
    "                        concepts  = ConceptsOptions(limit=20),\n",
    "                        entities  = EntitiesOptions(limit=20),\n",
    "                        keywords  = KeywordsOptions(limit=20),\n",
    "                        relations = RelationsOptions(),\n",
    "                        syntax    = SyntaxOptions()\n",
    "                        )\n",
    "\n",
    "nlu = NaLaUn(version=NLU['version'] , iam_apikey = NLU['apikey'], url = NLU['apiurl'])  #Local Natural Language Understanding object\n",
    "\n",
    "################\n",
    "## PREPARE DATA \n",
    "################\n",
    "filenames = os.listdir(PATH['data']) \n",
    "dictation_dic = {}            #dictionary for dictation files\n",
    "for name in filenames:\n",
    "    dictation_dic[name.replace('.txt','')] = open(PATH['data']+name).read()\n",
    "\n",
    "    \n",
    "# Treat dictations_dic as \n",
    "# - dict when type(key)=str, eg dictation_dic['12'] -> value for key '12'\n",
    "# - list when type(key)=int, eg dictation_dic[12] -> 12th value in dictionary \n",
    "def select_dictation(key):\n",
    "    if type(key) is int:\n",
    "        aa = list(dictation_dic.values())[key]\n",
    "    else:\n",
    "        aa = dictation_dic[key]\n",
    "    return aa\n",
    "dn = select_dictation           # dn <- Alias for select_dictation\n",
    "\n",
    "###############################\n",
    "## PERFORM WATSON NLU ANALYSIS\n",
    "###############################\n",
    "dictation_analysis = {}\n",
    "dian = dictation_analysis\n",
    "\n",
    "# If dictation_analysis dictionary already exists - read the pickled file\n",
    "# If it does NOT already exist, perform calculations. \n",
    "dian_pkl_file = PATH['results']+'all_dictations_nlu.pkl'  \n",
    "dian_pkl_exists = os.path.exists(dian_pkl_file)\n",
    "\n",
    "if dian_pkl_exists:\n",
    "    dian = pickle.load( open( dian_pkl_file, \"rb\" ) )\n",
    "\n",
    "else: #perform nlu-analysis on dictations\n",
    "    for item in list(dictation_dic.items()):\n",
    "        lbl  = item[0]\n",
    "        text = item[1]\n",
    "        dian[lbl] = nlu.analyze(text = text, features=NLU['features'])\n",
    "        f = open(PATH['results']+str(lbl)+'_nlu.pkl','wb')\n",
    "        pickle.dump(dian[lbl],f)\n",
    "        f.close()\n",
    "\n",
    "    f = open(dian_pkl_file,'wb')\n",
    "    pickle.dump(dian,f)\n",
    "    f.close()  \n",
    "\n",
    "# Transform dian to Pandas Dataframes\n",
    "df_dic = {}\n",
    "for dctn in dian.items():\n",
    "    df_dic[dctn[0]] = {}\n",
    "    for item in list(dctn[1].result.items()):\n",
    "        df_dic[dctn[0]][item[0]]=pd.DataFrame(list(item[1]))\n",
    "\n",
    "##############\n",
    "# ARCHETYPAL ANALYSIS\n",
    "##############\n",
    "\n",
    "# df = pd.DataFrame()\n",
    "\n",
    "# for key in df_dic:\n",
    "#     dfx = df_dic[key]['concepts'].copy()\n",
    "#     dfx['dictation'] = key\n",
    "#     df = df.append(dfx,sort=True)\n",
    "\n",
    "# mat = df.pivot('dictation','text','relevance')\n",
    "# m = mat.fillna(0)\n",
    "\n",
    "# archetypes = {}\n",
    "\n",
    "# n = 10     # Select number of Archetypes\n",
    "# mar = Archetypes(m,n) \n",
    "# archetypes[n] = {}\n",
    "# for i in range(n):\n",
    "#     archetypes[n][i] = mar.f.iloc[i].sort_values(ascending=False)\n",
    "\n",
    "\n",
    "def archs(typ,n=6):\n",
    "    if not 'archetype' in globals():\n",
    "        global archetype\n",
    "        archetype = {}\n",
    "    if not typ in archetype.keys():\n",
    "        archetype[typ] = {}\n",
    "    if not n in archetype[typ].keys():\n",
    "        archetype[typ][n] = {}\n",
    "        df = pd.DataFrame()\n",
    "        for key in df_dic:\n",
    "            dfx = df_dic[key][typ].copy()\n",
    "            dfx['dictation'] = key\n",
    "            df = df.append(dfx,sort=True)\n",
    "        if typ is 'entities':\n",
    "            df = df[df['type']=='HealthCondition']\n",
    "            df.rename({'relevance': 'rel0'}, axis=1,inplace=True)\n",
    "            df['relevance'] = df['rel0'] * df['confidence']\n",
    "        mat = df.pivot('dictation','text','relevance')\n",
    "        m = mat.fillna(0)\n",
    "        archetype[typ][n] = Archetypes(m,n)\n",
    "    return archetype[typ][n]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>text</th>\n",
       "      <th>ARDS</th>\n",
       "      <th>Addison's disease</th>\n",
       "      <th>Allergies</th>\n",
       "      <th>Allergies allergies</th>\n",
       "      <th>Alzheimer</th>\n",
       "      <th>Anemia</th>\n",
       "      <th>Anorexia</th>\n",
       "      <th>Atrial fibrillation</th>\n",
       "      <th>Bipolar disorder</th>\n",
       "      <th>Blood pressure</th>\n",
       "      <th>...</th>\n",
       "      <th>vascular disease</th>\n",
       "      <th>vascular encephalopathy</th>\n",
       "      <th>vein thrombosis</th>\n",
       "      <th>venous stasis ulcers</th>\n",
       "      <th>ventricular hypertrophy</th>\n",
       "      <th>vertigo</th>\n",
       "      <th>vitamin D deficiency</th>\n",
       "      <th>vomiting</th>\n",
       "      <th>wheezing</th>\n",
       "      <th>wrist fracture</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006038</td>\n",
       "      <td>0.015065</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003034</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.61346</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015315</td>\n",
       "      <td>0.009884</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000806</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009344</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002308</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.003326</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.007583</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023177</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002414</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038465</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018149</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.002654</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004086</td>\n",
       "      <td>0.033624</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028119</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042820</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050371</td>\n",
       "      <td>0.005454</td>\n",
       "      <td>0.014728</td>\n",
       "      <td>0.021669</td>\n",
       "      <td>0.002810</td>\n",
       "      <td>0.027488</td>\n",
       "      <td>0.000311</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.007817</td>\n",
       "      <td>0.005374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.491405</td>\n",
       "      <td>0.095658</td>\n",
       "      <td>0.191563</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002679</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.038446</td>\n",
       "      <td>0.012480</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025414</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020093</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022524</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 487 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "text      ARDS  Addison's disease  Allergies  Allergies allergies  Alzheimer  \\\n",
       "0     0.000000           0.006038   0.015065             0.000000   0.000000   \n",
       "1     0.000000           0.015315   0.009884             0.000000   0.000806   \n",
       "2     0.007583           0.000000   0.023177             0.000000   0.000000   \n",
       "3     0.002654           0.000000   0.004086             0.033624   0.000000   \n",
       "4     0.000000           0.000000   0.000000             0.000000   0.491405   \n",
       "5     0.000000           0.000000   0.000000             0.000000   0.000000   \n",
       "\n",
       "text    Anemia  Anorexia  Atrial fibrillation  Bipolar disorder  \\\n",
       "0     0.000000  0.000000             0.000000          0.000000   \n",
       "1     0.000000  0.000000             0.000000          0.000103   \n",
       "2     0.000000  0.000000             0.000000          0.002414   \n",
       "3     0.000000  0.000000             0.028119          0.000000   \n",
       "4     0.095658  0.191563             0.000000          0.002679   \n",
       "5     0.000000  0.038446             0.012480          0.000000   \n",
       "\n",
       "text  Blood pressure       ...        vascular disease  \\\n",
       "0           0.000000       ...                0.000000   \n",
       "1           0.000000       ...                0.000000   \n",
       "2           0.000000       ...                0.038465   \n",
       "3           0.042820       ...                0.050371   \n",
       "4           0.000000       ...                0.000000   \n",
       "5           0.025414       ...                0.020093   \n",
       "\n",
       "text  vascular encephalopathy  vein thrombosis  venous stasis ulcers  \\\n",
       "0                    0.003034         0.000000              0.000000   \n",
       "1                    0.000000         0.000000              0.000000   \n",
       "2                    0.000000         0.000000              0.018149   \n",
       "3                    0.005454         0.014728              0.021669   \n",
       "4                    0.000000         0.000000              0.000000   \n",
       "5                    0.000000         0.000000              0.000000   \n",
       "\n",
       "text  ventricular hypertrophy   vertigo  vitamin D deficiency  vomiting  \\\n",
       "0                    0.000000  0.000000              0.000000   0.61346   \n",
       "1                    0.009344  0.000000              0.002308   0.00000   \n",
       "2                    0.000000  0.000000              0.000000   0.00000   \n",
       "3                    0.002810  0.027488              0.000311   0.00000   \n",
       "4                    0.000000  0.000000              0.000000   0.00000   \n",
       "5                    0.000000  0.022524              0.000000   0.00000   \n",
       "\n",
       "text  wheezing  wrist fracture  \n",
       "0     0.000000        0.000000  \n",
       "1     0.003326        0.000000  \n",
       "2     0.000000        0.021970  \n",
       "3     0.007817        0.005374  \n",
       "4     0.000000        0.000000  \n",
       "5     0.000000        0.005025  \n",
       "\n",
       "[6 rows x 487 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "archs('entities',6).f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
